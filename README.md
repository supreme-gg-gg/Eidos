# Deep Learning Library from Scratch in C++

## Description

This is a project for educational purpose to implement MLP, CNN, RNN and variants and helper functions (e.g. data loader) from scratch using only Eigen for parallelized linear algebra in pure C++. The goal is to understand the underlying concepts of these models and how they work. The project is inspired by the libtorch, PyTorch's C++ frontend. We aim to provide a similar API interface to PyTorch and Keras for ease of use.

> The models are not optimized for performance like PyTorch. Thus, it will take (considerably) longer to train, especially for CNN. However, we gurantee that performance-wise, on tasks of reasonable complexity, it will be simliar to PyTorch.

## Getting Started

### Directory Structure

- `src/`: Contains the source code.
- `include/`: Contains header files.
- `tests/`: Contains unit tests.
- `build/`: Contains the build files (generated by CMake).

### How to Build

1. Clone the repo
2. Create a build directory: `mkdir build`
3. Navigate to `build/` and run: `cmake .. && make`

### Inspecting Sample

Run `make` in `sample` to see how an implementation using MLPack, an existing C++ ML framework, would look like. This is just for reference and not part of the main project.

### Running Tests

To run the unit tests, follow these steps:

1. Ensure you have built the project as described in the "How to Build" section.
2. Navigate to the `build/` directory.
3. Run the tests using the following command:

   ```sh
   make test
   ```

This project uses Google Test and ctest for unit testing.

## API Interface

### Convenience API

The following shows a simple training loop for a MLP model using the API interface. We assume that the input tensor and one-hot encoded targets are already defined. For reference for the convenience Tensor class, see `include/tensor.hpp`.

```cpp
Model model; // Creates a model class

// Add layers and activations
model.Add(new DenseLayer(input_size, 64));
model.Add(new ReLU());
model.Add(new DenseLayer(64, 32));
model.Add(new ReLU());
model.Add(new DenseLayer(32, output_size));

// Set optimizer and loss function
Adam optimizer(0.001);
CrossEntropyLoss loss_fn;
model.set_optimizer(optimizer);

// Train using the convenience API
model.Train(input_tensor, one_hot_targets, 50, loss_fn);

// Test using the convenience API
model.Test(input_tensor, one_hot_targets, loss_fn);
```

### Custom Training Loop

Custom training loop allows more flexibility in training the model and follows the PyTorch API interface.

```cpp
for (int epoch = 0; epoch < 10; ++epoch) {
   // Forward pass
   Tensor output = model.forward(input_tensor);

   // Compute loss
   float loss = loss_fn.forward(output, one_hot_targets);
   std::cout << "Epoch: " << epoch << " Loss: " << loss << std::endl;

   model.backward(grad); // Backprop through layers

   model.optimize(); // Update weights
}
```

### Callbacks

Callbacks are only supported natively with the built-in `model.Train()` method, which monitors trigger points during training. You may also implement your own callbacks by extending the `Callback` class.

```cpp
// Stop training if loss does not improve for 5 epochs
EarlyStopping early_stopping(5);

// Print loss every 10 epochs
PrintLoss print_loss(10);

// Add callbacks to the model
model.add_callback(&early_stopping);

// Train using the convenience API
model.Train(input_tensor, one_hot_targets, 50, loss_fn);
```

### Data Utilities

## Contributing

## Credits
